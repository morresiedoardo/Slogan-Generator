{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Slogan Generator with GPT-2"
      ],
      "metadata": {
        "id": "pRInkv0Tdo6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y transformers accelerate\n",
        "!pip install transformers accelerate"
      ],
      "metadata": {
        "id": "4WGYgZwIJ_iE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdq_NuCbqbBg",
        "outputId": "7970eb95-f300-417f-ce40-ec14324e8a73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the Tokenizer and Model."
      ],
      "metadata": {
        "id": "6oEFP76vdu5i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSLgzqNJJRZ8"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "MODEL_NAME = 'distilgpt2' #'distilgpt2' 'gpt2-medium'\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
        "model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Declare special tokens for padding and separating the context from the slogan then add these special tokens to the vocabulary and resize model's embeddings.\n"
      ],
      "metadata": {
        "id": "t2Rjy5wTdywj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SPECIAL_TOKENS_DICT = {\n",
        "    'pad_token': '<pad>',\n",
        "    'additional_special_tokens': ['<prompt>', '<output>', '<company>'],\n",
        "}\n",
        "\n",
        "tokenizer.add_special_tokens(SPECIAL_TOKENS_DICT)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "print(tokenizer.special_tokens_map)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9y9Y8RvMJcbc",
        "outputId": "d678d9be-2a61-4ad9-b787-76339301fa0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<pad>', 'additional_special_tokens': ['<prompt>', '<output>', '<company>']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a SloganDataset class as a child class of Torch's Dataset.\n",
        "\n",
        "1. The `SloganDataset` class inherits from the `torch.utils.data.Dataset` class and takes `filename`, `tokenizer`, and `seq_length` as input parameters.\n",
        "\n",
        "2. Inside the constructor `__init__()`, the additional special tokens and padding token are extracted from the tokenizer and assigned to variables: `prompt_tkn`, `output_tkn`, `pad_tkn`, and `eos_tkn`.\n",
        "\n",
        "3. For each row in the DataFrame, the prompt and output are tokenized using the tokenizer. The prompt tokens are prepended with the `prompt_tkn`, and the output tokens are prepended with `output_tkn` and appended with `eos_tkn`.\n",
        "\n",
        "4. The prompt tokens, output tokens, and padding tokens are concatenated to form the `tokens` list, ensuring a fixed length of `seq_length` by padding with `pad_tkn` if necessary.\n",
        "\n",
        "5. The `segments` list is created to annotate each token with its corresponding segment. The prompt tokens are assigned the `prompt_tkn`, and the output tokens are assigned the `output_tkn`.\n",
        "\n",
        "6. The `labels` list is created to represent the labels for each token. The prompt tokens, padding tokens, and the `<output>` token are ignored by setting their labels to -100, while the rest of the output tokens retain their respective values.\n",
        "\n",
        "7. The preprocessed example, consisting of `tokens`, `segments`, and `labels`, is appended to the `self.examples` list, which holds all the preprocessed examples.\n",
        "\n",
        "8. The `__len__()` method is implemented to return the length of the dataset, which is the number of examples stored in `self.examples`.\n",
        "\n",
        "9. The `__getitem__()` method is implemented to retrieve an item from the dataset given its index `item`. It returns a tensor representation of the example stored in `self.examples`."
      ],
      "metadata": {
        "id": "_mj3YZdeeGl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class SloganDataset(Dataset):\n",
        "  def __init__(self, df, tokenizer, seq_length=64):\n",
        "\n",
        "\n",
        "    prompt_tkn = tokenizer.additional_special_tokens_ids[0]\n",
        "    output_tkn = tokenizer.additional_special_tokens_ids[1]\n",
        "    pad_tkn = tokenizer.pad_token_id\n",
        "    eos_tkn = tokenizer.eos_token_id\n",
        "\n",
        "    self.examples = []\n",
        "    self.df = df\n",
        "\n",
        "    for _, row in self.df.iterrows():\n",
        "        prompt = [prompt_tkn] + tokenizer.encode(row['prompt'], max_length=seq_length//2-1)\n",
        "        output = [output_tkn] + tokenizer.encode(row['output'], max_length=seq_length//2-2) + [eos_tkn]\n",
        "\n",
        "        tokens = prompt + output + [pad_tkn] * ( seq_length - len(prompt) - len(output) )\n",
        "\n",
        "        segments = [prompt_tkn] * len(prompt) + [output_tkn] * ( seq_length - len(prompt) )\n",
        "\n",
        "        labels = [-100] * (len(prompt)+1) + output[1:] + [-100] * ( seq_length - len(prompt) - len(output) )\n",
        "\n",
        "        self.examples.append((tokens, segments, labels))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.examples)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    return torch.tensor(self.examples[item])\n"
      ],
      "metadata": {
        "id": "HmL3Mr4cJeve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv('all_slogans.csv', delimiter=\";\")\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "train, dev, test = np.split(df.sample(frac=1), [int(.8*len(df)), int(.9*len(df))])\n",
        "\n",
        "train_dataset = SloganDataset(train, tokenizer)\n",
        "dev_dataset = SloganDataset(dev, tokenizer)\n",
        "test_dataset = SloganDataset(test, tokenizer)\n",
        "\n",
        "print(next(iter(train_dataset)).size())\n",
        "print(next(iter(dev_dataset)).size())\n",
        "print(next(iter(test_dataset)).size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnyGoFYwUosJ",
        "outputId": "b64ffc2a-5cc7-4f3f-8e98-da17d8cfacab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 64])\n",
            "torch.Size([3, 64])\n",
            "torch.Size([3, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next block of code performs the following operations to create data loaders for training, validation, and testing splits:\n",
        "\n",
        "1. It imports the necessary modules from PyTorch to work with data loaders and samplers.\n",
        "\n",
        "2. The data indices for train, dev, and test splits are created using the `list(range(len(slogan_dataset)))` expression, assuming `slogan_dataset` is the dataset object.\n",
        "\n",
        "3. The indices list is shuffled randomly using `random.shuffle(indices)`, with a fixed random seed of 42 for reproducibility.\n",
        "\n",
        "4. The sizes for the dev and test splits are determined as percentages of the total dataset size using the `math.floor()` function. In this case, 10% of the dataset size is used for the dev split (`split_dev`), and 20% of the dataset size is used for the test split (`split_test`).\n",
        "\n",
        "5. The train, dev, and test indices are extracted based on the calculated split sizes using list slicing.\n",
        "\n",
        "6. PyTorch `SubsetRandomSampler` objects are created for each split, taking the respective indices as input. These samplers will sample elements randomly from the dataset based on the provided indices.\n",
        "\n",
        "7. Data loaders are built using the `DataLoader` class. The slogan dataset object (`slogan_dataset`) is passed as input, along with the batch size for each loader (`batch_size`).\n",
        "\n",
        "8. For the train loader, the `train_sampler` is used to sample batches randomly from the train split. The same process is followed for the dev and test loaders using their corresponding samplers (`dev_sampler` and `test_sampler`).\n",
        "\n",
        "9. The created train, dev, and test loaders (`train_loader`, `dev_loader`, and `test_loader`) can be used to iterate over the respective splits during the training, validation, and testing phases.\n",
        "\n",
        "NOTE: the batch size for validation can be doubled compared to training since no backpropagation is involved, which helps to fit the larger batch size into the GPU's memory.\n",
        "\n",
        "The code facilitates the splitting of the slogan dataset into train, dev, and test sets and prepares the data loaders for each split, enabling efficient iteration over the data during model training and evaluation."
      ],
      "metadata": {
        "id": "-ZttEQhIe4f0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import math, random\n",
        "from torch.utils.data import DataLoader\n",
        "# from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "# indices = list(range(len(slogan_dataset)))\n",
        "# random.seed(42)\n",
        "# random.shuffle(indices)\n",
        "\n",
        "# split_dev = math.floor(0.1 * len(slogan_dataset))\n",
        "# split_test = math.floor(0.2 * len(slogan_dataset))\n",
        "\n",
        "# train_indices = indices[split_test:]\n",
        "# dev_indices = indices[split_dev:split_test]\n",
        "# test_indices = indices[:split_dev]\n",
        "\n",
        "# train_sampler = SubsetRandomSampler(train_indices)\n",
        "# dev_sampler = SubsetRandomSampler(dev_indices)\n",
        "# test_sampler = SubsetRandomSampler(test_indices)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=64)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)"
      ],
      "metadata": {
        "id": "xgk54CqMJljQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next code defines a function called `fit` that trains the model using the provided optimizer and data loaders for training and validation.\n",
        "\n",
        "1. The `fit` function takes several input parameters: `model` (the model to be trained), `optimizer` (the optimizer for updating model parameters), `train_dl` (the data loader for the training set), `val_dl` (the data loader for the validation set), `epochs` (the number of training epochs), and `device` (the device to be used for training, with a default value of `torch.device('cpu')`).\n",
        "\n",
        "2. The function starts a loop over the specified number of epochs. Inside each epoch, it prints the epoch number.\n",
        "\n",
        "3. The model is set to training mode using `model.train()`.\n",
        "\n",
        "4. Two lists, `losses` and `nums`, are initialized to keep track of the batch losses and batch sizes over one epoch.\n",
        "\n",
        "5. The function iterates over the batches in the training data loader using `tqdm` for progress tracking.\n",
        "\n",
        "6. Each batch is moved to the specified device using `inputs = xb.to(device)`.\n",
        "\n",
        "7. The model is called with the input token IDs, segment IDs, and labels using `outputs = model(inputs[:,0,:], token_type_ids=inputs[:,1,:], labels=inputs[:,2,:])`.\n",
        "\n",
        "8. The loss is computed and added to the `losses` list, and the batch size is added to the `nums` list.\n",
        "\n",
        "9. Backpropagation is performed by calling `loss.backward()`.\n",
        "\n",
        "10. The optimizer takes a step to update the model parameters using `optimizer.step()`, and the model gradients are reset to zero using `model.zero_grad()`.\n",
        "\n",
        "11. After the epoch is finished, the average training cost is computed by multiplying the losses with the respective batch sizes, summing them, and dividing by the total number of samples in the epoch.\n",
        "\n",
        "12. The model is switched to evaluation mode using `model.eval()`.\n",
        "\n",
        "13. Validation is performed on the validation data loader using a similar process as training, but without backpropagation. The losses and batch sizes are stored in `losses` and `nums`, respectively.\n",
        "\n",
        "14. The average validation cost is computed in the same way as the training cost.\n",
        "\n",
        "15. Finally, the epoch number, training cost, and validation cost are printed."
      ],
      "metadata": {
        "id": "4LCQHIANfa8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def fit(model, optimizer, train_dl, val_dl, epochs=1, device=torch.device('cpu')):\n",
        "  for i in range(epochs):\n",
        "    print('\\n--- Starting epoch #{} ---'.format(i+1))\n",
        "    model.train()\n",
        "    losses = []\n",
        "    nums = []\n",
        "    for xb in tqdm(train_dl, desc=\"Training\"):\n",
        "      inputs = xb.to(device)\n",
        "      outputs = model(inputs[:,0,:], token_type_ids=inputs[:,1,:], labels=inputs[:,2,:])\n",
        "\n",
        "      loss = outputs[0]\n",
        "      losses.append(loss.item())\n",
        "      nums.append(len(xb))\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      model.zero_grad()\n",
        "\n",
        "    train_cost = np.sum(np.multiply(losses, nums)) / sum(nums)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      losses = []\n",
        "      nums = []\n",
        "      for xb in tqdm(val_dl, desc=\"Validation\"):\n",
        "        inputs = xb.to(device)\n",
        "        outputs = model(inputs[:,0,:], token_type_ids=inputs[:,1,:], labels=inputs[:,2,:])\n",
        "        losses.append(outputs[0].item())\n",
        "        nums.append(len(xb))\n",
        "\n",
        "    val_cost = np.sum(np.multiply(losses, nums)) / sum(nums)\n",
        "\n",
        "    print('\\n--- Epoch #{} finished --- Training cost: {} / Validation cost: {}'.format(i+1, round(train_cost, 3), round(val_cost, 3)))"
      ],
      "metadata": {
        "id": "Y0IWGAR0JrKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code block fine-tunes a GPT2 model for `n` epochs using the AdamW optimizer. Here's an overview of the code:\n",
        "\n",
        "1. It creates a `torch.device` object named `device` with the device type set to `'cuda'`, indicating that the GPU will be used for training.\n",
        "\n",
        "2. The `model` is moved to the GPU by calling `model.to(device)`, which ensures that the model's parameters and computations will be performed on the GPU.\n",
        "\n",
        "3. An instance of the AdamW optimizer is created by passing `model.parameters()` as the parameter. This optimizer will update the parameters of the `model` during training.\n",
        "\n",
        "4. The `fit` function is called with the following arguments: `model` (the GPT2 model), `optimizer` (the AdamW optimizer), `train_loader` (the data loader for the training set), `dev_loader` (the data loader for the validation set), `epochs=2` (indicating two training epochs), and `device=device` (specifying the device to be used for training)."
      ],
      "metadata": {
        "id": "VjMmINVRg4Xz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW\n",
        "\n",
        "device = torch.device('cuda')\n",
        "model.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), no_deprecation_warning=True)\n",
        "fit(model, optimizer, train_loader, dev_loader, epochs=5, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liKPCFrYJuDy",
        "outputId": "99e8d011-1ebd-4f21-fbc2-069dc4ab0212"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting epoch #1 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 291/291 [01:34<00:00,  3.07it/s]\n",
            "Validation: 100%|██████████| 19/19 [00:04<00:00,  4.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch #1 finished --- Training cost: 3.738 / Validation cost: 2.767\n",
            "\n",
            "--- Starting epoch #2 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 291/291 [01:35<00:00,  3.04it/s]\n",
            "Validation: 100%|██████████| 19/19 [00:04<00:00,  4.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch #2 finished --- Training cost: 2.285 / Validation cost: 2.862\n",
            "\n",
            "--- Starting epoch #3 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 291/291 [01:36<00:00,  3.03it/s]\n",
            "Validation: 100%|██████████| 19/19 [00:04<00:00,  4.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch #3 finished --- Training cost: 1.708 / Validation cost: 3.141\n",
            "\n",
            "--- Starting epoch #4 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 291/291 [01:35<00:00,  3.03it/s]\n",
            "Validation: 100%|██████████| 19/19 [00:04<00:00,  4.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch #4 finished --- Training cost: 1.269 / Validation cost: 3.459\n",
            "\n",
            "--- Starting epoch #5 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 291/291 [01:35<00:00,  3.03it/s]\n",
            "Validation: 100%|██████████| 19/19 [00:04<00:00,  4.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch #5 finished --- Training cost: 0.979 / Validation cost: 3.795\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the model after the training."
      ],
      "metadata": {
        "id": "yIMYgUJXd6zm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = '/content/model.pt'\n",
        "torch.save(model.state_dict(), model_path)"
      ],
      "metadata": {
        "id": "8XNAD_bzd2dO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save all the datasets."
      ],
      "metadata": {
        "id": "XATbqBK8wy4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train.to_csv('train.csv', sep=';')\n",
        "dev.to_csv('dev.csv', sep=';')\n",
        "test.to_csv('test.csv', sep=';')"
      ],
      "metadata": {
        "id": "GpMsJiVow0mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the saved model to avoid retraining."
      ],
      "metadata": {
        "id": "MDlecFUQd_1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "model_path = '/content/drive/MyDrive/model.pt'\n",
        "state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n",
        "model.load_state_dict(state_dict, model_path)\n",
        "device = torch.device('cuda')\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "mF1X40u8d_g4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20c1900e-a826-4452-fd48-94f242f6229e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50261, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-5): 6 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50261, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the datasets."
      ],
      "metadata": {
        "id": "dZS48TQOw_u1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train = pd.read_csv('train.csv', sep=';')\n",
        "dev = pd.read_csv('dev.csv', sep=';')\n",
        "test = pd.read_csv('test.csv', sep=';')"
      ],
      "metadata": {
        "id": "7eESkdPlxBVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `top_k_top_p_filtering` function defined below implements a filtering mechanism for probability distributions of logits using top-k and/or nucleus (top-p) filtering techniques. Let's break down the code and explain the idea behind this filtering:\n",
        "\n",
        "1. `top_k = min(top_k, logits.size(-1))`: This line ensures that `top_k` is not greater than the size of the logits distribution, which represents the vocabulary size. It performs a safety check to prevent errors.\n",
        "\n",
        "2. Top-k Filtering:\n",
        "   - `if top_k > 0`: This condition checks if top-k filtering is enabled.\n",
        "   - `indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]`: This line finds the `top_k`-th largest value in the logits and creates a Boolean mask indicating the tokens with probabilities less than this threshold. Tokens with probabilities below the threshold will be removed.\n",
        "   - `logits[indices_to_remove] = filter_value`: This sets the logits of the filtered tokens to `filter_value`, which is typically negative infinity. This effectively removes the tokens with lower probabilities from the distribution.\n",
        "\n",
        "3. Nucleus (Top-p) Filtering:\n",
        "   - `if top_p > 0.0`: This condition checks if nucleus filtering is enabled.\n",
        "   - `sorted_logits, sorted_indices = torch.sort(logits, descending=True)`: This line sorts the logits in descending order and returns the sorted logits and their corresponding indices.\n",
        "   - `cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)`: Here, the softmax is applied to the sorted logits, and the cumulative probabilities are computed along the last dimension.\n",
        "   - `sorted_indices_to_remove = cumulative_probs > top_p`: This line creates a Boolean mask indicating the tokens with cumulative probabilities above the `top_p` threshold.\n",
        "   - `sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()`: This line shifts the indices to the right, effectively removing the first token above the threshold but keeping the rest.\n",
        "   - `sorted_indices_to_remove[..., 0] = 0`: This sets the first token above the threshold to 0, ensuring it is not removed.\n",
        "   - `indices_to_remove = sorted_indices_to_remove.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_remove)`: This scatters the sorted indices back to their original positions, allowing the filtering of the original logits distribution.\n",
        "   - `logits[indices_to_remove] = filter_value`: Similar to top-k filtering, this line sets the logits of the filtered tokens to `filter_value`, effectively removing them from the distribution.\n",
        "\n",
        "Finally, the function returns the filtered logits distribution. The filtering mechanism helps control the diversity and randomness of the generated text by constraining the distribution based on the top-k tokens and/or cumulative probability threshold (top-p)."
      ],
      "metadata": {
        "id": "GQTxTbk5peFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
        "    top_k = min(top_k, logits.size(-1))\n",
        "    if top_k > 0:\n",
        "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    if top_p > 0.0:\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        indices_to_remove = sorted_indices_to_remove.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_remove)\n",
        "        logits[indices_to_remove] = filter_value\n",
        "    return logits"
      ],
      "metadata": {
        "id": "kTEUV-0XJzxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `sample_sequence` function generates slogans using GPT-2 by sampling tokens sequentially. Let's break down the code and explain its functionality:\n",
        "\n",
        "1. `prompt = torch.tensor(prompt, dtype=torch.long, device=device)`: This line converts the `prompt` into a PyTorch tensor of type `torch.long` and moves it to the GPU.\n",
        "\n",
        "2. `prompt = prompt.unsqueeze(0).repeat(num_samples, 1)`: The `prompt` tensor is unsqueezed to add a batch dimension of size 1, and then repeated `num_samples` times to create multiple independent sequences for generating multiple slogans simultaneously.\n",
        "\n",
        "3. `generated = prompt`: The `generated` tensor is initialized with the prompt, which serves as the starting point for generating the slogans.\n",
        "\n",
        "4. The generation process begins with a loop that iterates `length` times to generate the desired number of tokens for each slogan:\n",
        "\n",
        "   - `inputs = {'input_ids': generated}`: The `generated` tensor is used as the input to the model. It contains the token IDs of the previously generated tokens.\n",
        "   \n",
        "   - `if segments_tokens != None: ...`: If `segments_tokens` is provided, it represents the segment or context information for the input tokens. It is used to create the 'token_type_ids' tensor that specifies the segment information for each token in `generated`.\n",
        "\n",
        "   - `outputs = model(**inputs)`: The model is called with the `inputs` dictionary to obtain the model outputs. The specific details may vary depending on the model architecture.\n",
        "\n",
        "   - `next_token_logits = outputs[0][:, -1, :] / (temperature if temperature > 0 else 1.)`: The logits for the next token in the sequence are extracted from the model outputs. The logits are divided by the temperature value to control the randomness of the sampling. If `temperature` is 0, greedy sampling is performed by choosing the token with the highest probability.\n",
        "\n",
        "   - `for i in range(num_samples): ...`: For each individual slogan sequence, a repetition penalty is applied to discourage repeated tokens. The logits for each token that already exists in the generated sequence are divided by the `repetition_penalty` value.\n",
        "\n",
        "   - `filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)`: The `top_k_top_p_filtering` function is applied to the logits to filter the distribution based on the top-k and/or nucleus (top-p) filtering techniques. This helps control the diversity and randomness of the generated slogans.\n",
        "\n",
        "   - `if temperature == 0: ...`: If `temperature` is 0, greedy sampling is performed by selecting the token with the highest probability (argmax) from the filtered logits. The selected token is unsqueezed and added to the `generated` tensor.\n",
        "\n",
        "   - `else: ...`: For non-zero `temperature` values, multinomial sampling is performed using the filtered logits and the `num_samples` parameter. The sampled token is unsqueezed and added to the `generated` tensor.\n",
        "\n",
        "5. Finally, the function returns the `generated` tensor, which contains the generated slogans."
      ],
      "metadata": {
        "id": "RfocSyPCqUK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from tqdm import trange\n",
        "\n",
        "def sample_sequence(model, length, prompt, segments_tokens=None, num_samples=1, temperature=1, top_k=0, top_p=0.0, repetition_penalty=1.0,\n",
        "                    device='cpu'):\n",
        "    prompt = torch.tensor(prompt, dtype=torch.long, device=device)\n",
        "    prompt = prompt.unsqueeze(0).repeat(num_samples, 1)\n",
        "    generated = prompt\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # for _ in trange(length, leave=True):\n",
        "        for _ in range(length):\n",
        "            inputs = {'input_ids': generated}\n",
        "            if segments_tokens != None:\n",
        "              inputs['token_type_ids'] = torch.tensor(segments_tokens[:generated.shape[1]]).unsqueeze(0).repeat(num_samples, 1)\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            next_token_logits = outputs[0][:, -1, :] / (temperature if temperature > 0 else 1.)\n",
        "\n",
        "            for i in range(num_samples):\n",
        "                for _ in set(generated[i].tolist()):\n",
        "                    next_token_logits[i, _] /= repetition_penalty\n",
        "\n",
        "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
        "            if temperature == 0:\n",
        "                next_token = torch.argmax(filtered_logits, dim=-1).unsqueeze(-1)\n",
        "            else:\n",
        "                next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
        "            generated = torch.cat((generated, next_token), dim=1)\n",
        "\n",
        "    # trange.close()\n",
        "\n",
        "    return generated"
      ],
      "metadata": {
        "id": "s6w9_J9wparT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Coca-Cola, Drinking\"\n",
        "\n",
        "prompt_tkn = tokenizer.additional_special_tokens_ids[0]\n",
        "output_tkn = tokenizer.additional_special_tokens_ids[1]\n",
        "\n",
        "input_ids = [prompt_tkn] + tokenizer.encode(prompt)\n",
        "\n",
        "segments = [output_tkn] * 64\n",
        "segments[:len(input_ids)] = [prompt_tkn] * len(input_ids)\n",
        "\n",
        "input_ids += [output_tkn]\n",
        "\n",
        "model.to(torch.device('cpu'))\n",
        "\n",
        "generated = sample_sequence(model, length=20, prompt=input_ids, segments_tokens=segments, num_samples=20)\n",
        "\n",
        "print('\\n--- Generated Slogans ---\\n')\n",
        "\n",
        "for g in generated:\n",
        "  slogan = tokenizer.decode(g.squeeze().tolist())\n",
        "  slogan = slogan.split('<|endoftext|>')[0].split('<output>')[1]\n",
        "  print(slogan)"
      ],
      "metadata": {
        "id": "agQcp6LvJ4ck",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ba5ed0d-86ad-4c22-a76e-9f3639a47fa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:24<00:00,  1.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Generated Slogans ---\n",
            "\n",
            " Coca-Cola, Breathing life.\n",
            " Coca-Cola, Taste it everything.\n",
            " Coca-Cola, Taste the possibilities.\n",
            " Coca-Cola, Taste the possibilities.\n",
            " Coca-Cola, Just chill.\n",
            " Coca-Cola, Taste the possibilities.\n",
            " Coca-Cola, Just try and Smirk.\n",
            " Coca-Cola, Taste the possibilities.\n",
            " Coca-Cola, Taste the possibilities.\n",
            " Coca-Cola, Taste it together.\n",
            " Coca-Cola, Welcome to the World Cup Cup.\n",
            " Coca-Cola, Have fun in it.\n",
            " Coca-Cola, What's for breakfast like today?\n",
            " Coca-Cola, Taste the possibilities.\n",
            " Coca-Cola, Try theCola. Try the World.\n",
            " Coca-Cola, Taste the mystery.\n",
            " Coca-Cola, Have you heard the Pepsi-Cola test of it?\n",
            " Coca-Cola, Taste it all.\n",
            " Coca-Cola, Brew the Pepsi, Think of it as a Coke.\n",
            " Coca-Cola, Smell it. Don't believe it.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test.drop(['output', 'language'], axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "hCIz9HP1axwk",
        "outputId": "269f5ae6-17ae-4d18-e88b-170594b82841"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   company    category  \\\n",
              "1435  The National Lottery      Sports   \n",
              "529            Minute Maid    Drinking   \n",
              "8898           P&O Ferries       Tours   \n",
              "1497           Holiday Inn       Tours   \n",
              "7097              Best Buy    Business   \n",
              "...                    ...         ...   \n",
              "2594         Disney Junior    Business   \n",
              "8192                 Intel  Technology   \n",
              "4695              Emirates    Airlines   \n",
              "8992      Woodford Reserve    Drinking   \n",
              "4615              Café Hag    Drinking   \n",
              "\n",
              "                                          slogan  \\\n",
              "1435                             It could be you   \n",
              "529                           For moms who know.   \n",
              "8898                      You deserve a holiday.   \n",
              "1497           We put a smile back on your face.   \n",
              "7097                            Turn on the fun.   \n",
              "...                                          ...   \n",
              "2594                   Welcome to Disney Junior!   \n",
              "8192                        The Computer Inside.   \n",
              "4695                  Fly us once, fly us always   \n",
              "8992               Handcrafted Kentucky bourbon.   \n",
              "4615  With coffee this good, who needs caffeine?   \n",
              "\n",
              "                                   slogan_masked                        prompt  \n",
              "1435                             It could be you  The National Lottery, Sports  \n",
              "529                           For moms who know.         Minute Maid, Drinking  \n",
              "8898                      You deserve a holiday.            P&O Ferries, Tours  \n",
              "1497           We put a smile back on your face.            Holiday Inn, Tours  \n",
              "7097                            Turn on the fun.            Best Buy, Business  \n",
              "...                                          ...                           ...  \n",
              "2594                       Welcome to <company>!       Disney Junior, Business  \n",
              "8192                        The Computer Inside.             Intel, Technology  \n",
              "4695                  Fly us once, fly us always            Emirates, Airlines  \n",
              "8992               Handcrafted Kentucky bourbon.    Woodford Reserve, Drinking  \n",
              "4615  With coffee this good, who needs caffeine?            Café Hag, Drinking  \n",
              "\n",
              "[1162 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-52a47259-77ed-4a9f-9005-6ce5c7807adf\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>company</th>\n",
              "      <th>category</th>\n",
              "      <th>slogan</th>\n",
              "      <th>slogan_masked</th>\n",
              "      <th>prompt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1435</th>\n",
              "      <td>The National Lottery</td>\n",
              "      <td>Sports</td>\n",
              "      <td>It could be you</td>\n",
              "      <td>It could be you</td>\n",
              "      <td>The National Lottery, Sports</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>529</th>\n",
              "      <td>Minute Maid</td>\n",
              "      <td>Drinking</td>\n",
              "      <td>For moms who know.</td>\n",
              "      <td>For moms who know.</td>\n",
              "      <td>Minute Maid, Drinking</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8898</th>\n",
              "      <td>P&amp;O Ferries</td>\n",
              "      <td>Tours</td>\n",
              "      <td>You deserve a holiday.</td>\n",
              "      <td>You deserve a holiday.</td>\n",
              "      <td>P&amp;O Ferries, Tours</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1497</th>\n",
              "      <td>Holiday Inn</td>\n",
              "      <td>Tours</td>\n",
              "      <td>We put a smile back on your face.</td>\n",
              "      <td>We put a smile back on your face.</td>\n",
              "      <td>Holiday Inn, Tours</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7097</th>\n",
              "      <td>Best Buy</td>\n",
              "      <td>Business</td>\n",
              "      <td>Turn on the fun.</td>\n",
              "      <td>Turn on the fun.</td>\n",
              "      <td>Best Buy, Business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2594</th>\n",
              "      <td>Disney Junior</td>\n",
              "      <td>Business</td>\n",
              "      <td>Welcome to Disney Junior!</td>\n",
              "      <td>Welcome to &lt;company&gt;!</td>\n",
              "      <td>Disney Junior, Business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8192</th>\n",
              "      <td>Intel</td>\n",
              "      <td>Technology</td>\n",
              "      <td>The Computer Inside.</td>\n",
              "      <td>The Computer Inside.</td>\n",
              "      <td>Intel, Technology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4695</th>\n",
              "      <td>Emirates</td>\n",
              "      <td>Airlines</td>\n",
              "      <td>Fly us once, fly us always</td>\n",
              "      <td>Fly us once, fly us always</td>\n",
              "      <td>Emirates, Airlines</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8992</th>\n",
              "      <td>Woodford Reserve</td>\n",
              "      <td>Drinking</td>\n",
              "      <td>Handcrafted Kentucky bourbon.</td>\n",
              "      <td>Handcrafted Kentucky bourbon.</td>\n",
              "      <td>Woodford Reserve, Drinking</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4615</th>\n",
              "      <td>Café Hag</td>\n",
              "      <td>Drinking</td>\n",
              "      <td>With coffee this good, who needs caffeine?</td>\n",
              "      <td>With coffee this good, who needs caffeine?</td>\n",
              "      <td>Café Hag, Drinking</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1162 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-52a47259-77ed-4a9f-9005-6ce5c7807adf')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-52a47259-77ed-4a9f-9005-6ce5c7807adf button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-52a47259-77ed-4a9f-9005-6ce5c7807adf');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import trange\n",
        "\n",
        "def generate_slogans_for_column(prompt, model, tokenizer):\n",
        "    prompt_tkn = tokenizer.additional_special_tokens_ids[0]\n",
        "    output_tkn = tokenizer.additional_special_tokens_ids[1]\n",
        "\n",
        "    input_ids = [prompt_tkn] + tokenizer.encode(prompt)\n",
        "    segments = [output_tkn] * 64\n",
        "    segments[:len(input_ids)] = [prompt_tkn] * len(input_ids)\n",
        "    input_ids += [output_tkn]\n",
        "\n",
        "    model.to(torch.device('cpu'))\n",
        "\n",
        "    generated = sample_sequence(model, length=20, prompt=input_ids, segments_tokens=segments, num_samples=1)\n",
        "\n",
        "    # slogans = []\n",
        "    # for g in generated:\n",
        "    slogan = tokenizer.decode(generated.squeeze().tolist())\n",
        "    slogan = slogan.split('<|endoftext|>')[0].split('<output>')[1]\n",
        "    return slogan[1:]\n",
        "\n",
        "def generate_slogans_for_dataset(data_column, model, tokenizer):\n",
        "    generated_slogans = []\n",
        "    with trange(len(data_column), leave=True) as pbar:\n",
        "        for prompt in data_column:\n",
        "            slogans = generate_slogans_for_column(prompt, model, tokenizer)\n",
        "            generated_slogans.append(slogans)\n",
        "            pbar.update(1)\n",
        "    return generated_slogans"
      ],
      "metadata": {
        "id": "ZJAhUHI6sSdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_slogans_for_dataset(test['prompt'][:5], model, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FaXU4Rdsdfd",
        "outputId": "0b084435-9fa6-4b69-971d-6ae6bce2bb28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:12<00:00,  2.47s/it]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The National Lottery, Buy the lottery ticket at a big and savings',\n",
              " 'Minute Maid, Someone had their Minute Maid today.',\n",
              " 'P&O Ferries, Here to Brittany with us.',\n",
              " 'Holiday Inn, Hotels with style. Red Roofed in a Holiday Inn.',\n",
              " 'Best Buy, Time, money, and technology.']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test['output_generated'] = generate_slogans_for_dataset(test['prompt'], model, tokenizer)"
      ],
      "metadata": {
        "id": "2uxQphOKacG3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baee0080-c624-4925-8a8a-8862662f4032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1162/1162 [37:50<00:00,  1.95s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test"
      ],
      "metadata": {
        "id": "bFCYQC94apQ_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        },
        "outputId": "e321f9e1-7fd2-4ad2-9b96-567ed923ca11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Unnamed: 0               company    category  \\\n",
              "0           1435  The National Lottery      Sports   \n",
              "1            529           Minute Maid    Drinking   \n",
              "2           8898           P&O Ferries       Tours   \n",
              "3           1497           Holiday Inn       Tours   \n",
              "4           7097              Best Buy    Business   \n",
              "...          ...                   ...         ...   \n",
              "1157        2594         Disney Junior    Business   \n",
              "1158        8192                 Intel  Technology   \n",
              "1159        4695              Emirates    Airlines   \n",
              "1160        8992      Woodford Reserve    Drinking   \n",
              "1161        4615              Café Hag    Drinking   \n",
              "\n",
              "                                          slogan language  \\\n",
              "0                                It could be you       en   \n",
              "1                             For moms who know.       en   \n",
              "2                         You deserve a holiday.       en   \n",
              "3              We put a smile back on your face.       en   \n",
              "4                               Turn on the fun.       en   \n",
              "...                                          ...      ...   \n",
              "1157                   Welcome to Disney Junior!       en   \n",
              "1158                        The Computer Inside.       en   \n",
              "1159                  Fly us once, fly us always       en   \n",
              "1160               Handcrafted Kentucky bourbon.       en   \n",
              "1161  With coffee this good, who needs caffeine?       en   \n",
              "\n",
              "                                   slogan_masked  \\\n",
              "0                                It could be you   \n",
              "1                             For moms who know.   \n",
              "2                         You deserve a holiday.   \n",
              "3              We put a smile back on your face.   \n",
              "4                               Turn on the fun.   \n",
              "...                                          ...   \n",
              "1157                       Welcome to <company>!   \n",
              "1158                        The Computer Inside.   \n",
              "1159                  Fly us once, fly us always   \n",
              "1160               Handcrafted Kentucky bourbon.   \n",
              "1161  With coffee this good, who needs caffeine?   \n",
              "\n",
              "                            prompt  \\\n",
              "0     The National Lottery, Sports   \n",
              "1            Minute Maid, Drinking   \n",
              "2               P&O Ferries, Tours   \n",
              "3               Holiday Inn, Tours   \n",
              "4               Best Buy, Business   \n",
              "...                            ...   \n",
              "1157       Disney Junior, Business   \n",
              "1158             Intel, Technology   \n",
              "1159            Emirates, Airlines   \n",
              "1160    Woodford Reserve, Drinking   \n",
              "1161            Café Hag, Drinking   \n",
              "\n",
              "                                                 output  \\\n",
              "0                 The National Lottery, It could be you   \n",
              "1                       Minute Maid, For moms who know.   \n",
              "2                   P&O Ferries, You deserve a holiday.   \n",
              "3        Holiday Inn, We put a smile back on your face.   \n",
              "4                            Best Buy, Turn on the fun.   \n",
              "...                                                 ...   \n",
              "1157           Disney Junior, Welcome to Disney Junior!   \n",
              "1158                        Intel, The Computer Inside.   \n",
              "1159               Emirates, Fly us once, fly us always   \n",
              "1160    Woodford Reserve, Handcrafted Kentucky bourbon.   \n",
              "1161  Café Hag, With coffee this good, who needs caf...   \n",
              "\n",
              "                                       output_generated  \n",
              "0     The National Lottery, Play the National Lotter...  \n",
              "1                  Minute Maid, Prettier than the Maid.  \n",
              "2                    P&O Ferries, Where you come first.  \n",
              "3     Holiday Inn, It over the world'sHoliday Innkee...  \n",
              "4           Best Buy, Expert Service. Streetwise Price.  \n",
              "...                                                 ...  \n",
              "1157             Disney Junior, Where the Magic Begins!  \n",
              "1158                       Intel, Sponsors of tomorrow.  \n",
              "1159        Emirates, Fly the airline of the year 2005.  \n",
              "1160  Woodford Reserve, Woodford Reserve Reclass <co...  \n",
              "1161  Café Hag, You can't knock them over a Cold. Tr...  \n",
              "\n",
              "[1162 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a85be77b-bd44-4126-abc8-af5e135a7fc9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>company</th>\n",
              "      <th>category</th>\n",
              "      <th>slogan</th>\n",
              "      <th>language</th>\n",
              "      <th>slogan_masked</th>\n",
              "      <th>prompt</th>\n",
              "      <th>output</th>\n",
              "      <th>output_generated</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1435</td>\n",
              "      <td>The National Lottery</td>\n",
              "      <td>Sports</td>\n",
              "      <td>It could be you</td>\n",
              "      <td>en</td>\n",
              "      <td>It could be you</td>\n",
              "      <td>The National Lottery, Sports</td>\n",
              "      <td>The National Lottery, It could be you</td>\n",
              "      <td>The National Lottery, Play the National Lotter...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>529</td>\n",
              "      <td>Minute Maid</td>\n",
              "      <td>Drinking</td>\n",
              "      <td>For moms who know.</td>\n",
              "      <td>en</td>\n",
              "      <td>For moms who know.</td>\n",
              "      <td>Minute Maid, Drinking</td>\n",
              "      <td>Minute Maid, For moms who know.</td>\n",
              "      <td>Minute Maid, Prettier than the Maid.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8898</td>\n",
              "      <td>P&amp;O Ferries</td>\n",
              "      <td>Tours</td>\n",
              "      <td>You deserve a holiday.</td>\n",
              "      <td>en</td>\n",
              "      <td>You deserve a holiday.</td>\n",
              "      <td>P&amp;O Ferries, Tours</td>\n",
              "      <td>P&amp;O Ferries, You deserve a holiday.</td>\n",
              "      <td>P&amp;O Ferries, Where you come first.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1497</td>\n",
              "      <td>Holiday Inn</td>\n",
              "      <td>Tours</td>\n",
              "      <td>We put a smile back on your face.</td>\n",
              "      <td>en</td>\n",
              "      <td>We put a smile back on your face.</td>\n",
              "      <td>Holiday Inn, Tours</td>\n",
              "      <td>Holiday Inn, We put a smile back on your face.</td>\n",
              "      <td>Holiday Inn, It over the world'sHoliday Innkee...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7097</td>\n",
              "      <td>Best Buy</td>\n",
              "      <td>Business</td>\n",
              "      <td>Turn on the fun.</td>\n",
              "      <td>en</td>\n",
              "      <td>Turn on the fun.</td>\n",
              "      <td>Best Buy, Business</td>\n",
              "      <td>Best Buy, Turn on the fun.</td>\n",
              "      <td>Best Buy, Expert Service. Streetwise Price.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1157</th>\n",
              "      <td>2594</td>\n",
              "      <td>Disney Junior</td>\n",
              "      <td>Business</td>\n",
              "      <td>Welcome to Disney Junior!</td>\n",
              "      <td>en</td>\n",
              "      <td>Welcome to &lt;company&gt;!</td>\n",
              "      <td>Disney Junior, Business</td>\n",
              "      <td>Disney Junior, Welcome to Disney Junior!</td>\n",
              "      <td>Disney Junior, Where the Magic Begins!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1158</th>\n",
              "      <td>8192</td>\n",
              "      <td>Intel</td>\n",
              "      <td>Technology</td>\n",
              "      <td>The Computer Inside.</td>\n",
              "      <td>en</td>\n",
              "      <td>The Computer Inside.</td>\n",
              "      <td>Intel, Technology</td>\n",
              "      <td>Intel, The Computer Inside.</td>\n",
              "      <td>Intel, Sponsors of tomorrow.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1159</th>\n",
              "      <td>4695</td>\n",
              "      <td>Emirates</td>\n",
              "      <td>Airlines</td>\n",
              "      <td>Fly us once, fly us always</td>\n",
              "      <td>en</td>\n",
              "      <td>Fly us once, fly us always</td>\n",
              "      <td>Emirates, Airlines</td>\n",
              "      <td>Emirates, Fly us once, fly us always</td>\n",
              "      <td>Emirates, Fly the airline of the year 2005.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1160</th>\n",
              "      <td>8992</td>\n",
              "      <td>Woodford Reserve</td>\n",
              "      <td>Drinking</td>\n",
              "      <td>Handcrafted Kentucky bourbon.</td>\n",
              "      <td>en</td>\n",
              "      <td>Handcrafted Kentucky bourbon.</td>\n",
              "      <td>Woodford Reserve, Drinking</td>\n",
              "      <td>Woodford Reserve, Handcrafted Kentucky bourbon.</td>\n",
              "      <td>Woodford Reserve, Woodford Reserve Reclass &lt;co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1161</th>\n",
              "      <td>4615</td>\n",
              "      <td>Café Hag</td>\n",
              "      <td>Drinking</td>\n",
              "      <td>With coffee this good, who needs caffeine?</td>\n",
              "      <td>en</td>\n",
              "      <td>With coffee this good, who needs caffeine?</td>\n",
              "      <td>Café Hag, Drinking</td>\n",
              "      <td>Café Hag, With coffee this good, who needs caf...</td>\n",
              "      <td>Café Hag, You can't knock them over a Cold. Tr...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1162 rows × 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a85be77b-bd44-4126-abc8-af5e135a7fc9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a85be77b-bd44-4126-abc8-af5e135a7fc9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a85be77b-bd44-4126-abc8-af5e135a7fc9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test.to_csv('test_generated.csv', sep=';')"
      ],
      "metadata": {
        "id": "LthIRKY2k5zK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_gen = pd.read_csv('test_generated.csv', sep=';')"
      ],
      "metadata": {
        "id": "ROmqzvvB0_YP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recombine the three datasets"
      ],
      "metadata": {
        "id": "oSWuB8rskyQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_copy = test_gen.drop('output', axis=1).rename(columns={'output_generated':'output'})\n",
        "\n",
        "test_copy['generated'] = 1\n",
        "train['generated'] = 0\n",
        "dev['generated'] = 0\n",
        "\n",
        "df_class = pd.concat([train, dev, test_copy])"
      ],
      "metadata": {
        "id": "zFbgkAIEk0fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_class.drop(['Unnamed: 0', 'Unnamed: 0.1'], axis=1, inplace=True)\n",
        "df_class.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "KxyaUNeU4HN3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        },
        "outputId": "fe7bb5b1-64dd-4f93-e842-3d2d82e66b82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                company    category  \\\n",
              "0         Massachusetts       Tours   \n",
              "1       Café Coffee Day    Drinking   \n",
              "2              Colorado       Tours   \n",
              "3                Lipton    Drinking   \n",
              "4                 Boost        Food   \n",
              "...                 ...         ...   \n",
              "11613     Disney Junior    Business   \n",
              "11614             Intel  Technology   \n",
              "11615          Emirates    Airlines   \n",
              "11616  Woodford Reserve    Drinking   \n",
              "11617          Café Hag    Drinking   \n",
              "\n",
              "                                                  slogan language  \\\n",
              "0                        Massachusetts: where God is God       en   \n",
              "1      We’re Brewing A Marathon Of Celebrations This ...       en   \n",
              "2                                  Rocky Mountain Empire       en   \n",
              "3                                  Start something good.       en   \n",
              "4                         Boost Guarana: One Step Ahead.       en   \n",
              "...                                                  ...      ...   \n",
              "11613                          Welcome to Disney Junior!       en   \n",
              "11614                               The Computer Inside.       en   \n",
              "11615                         Fly us once, fly us always       en   \n",
              "11616                      Handcrafted Kentucky bourbon.       en   \n",
              "11617         With coffee this good, who needs caffeine?       en   \n",
              "\n",
              "                                           slogan_masked  \\\n",
              "0                            <company>: where God is God   \n",
              "1      We’re Brewing A Marathon Of Celebrations This ...   \n",
              "2                                  Rocky Mountain Empire   \n",
              "3                                  Start something good.   \n",
              "4                     <company> Guarana: One Step Ahead.   \n",
              "...                                                  ...   \n",
              "11613                              Welcome to <company>!   \n",
              "11614                               The Computer Inside.   \n",
              "11615                         Fly us once, fly us always   \n",
              "11616                      Handcrafted Kentucky bourbon.   \n",
              "11617         With coffee this good, who needs caffeine?   \n",
              "\n",
              "                           prompt  \\\n",
              "0            Massachusetts, Tours   \n",
              "1       Café Coffee Day, Drinking   \n",
              "2                 Colorado, Tours   \n",
              "3                Lipton, Drinking   \n",
              "4                     Boost, Food   \n",
              "...                           ...   \n",
              "11613     Disney Junior, Business   \n",
              "11614           Intel, Technology   \n",
              "11615          Emirates, Airlines   \n",
              "11616  Woodford Reserve, Drinking   \n",
              "11617          Café Hag, Drinking   \n",
              "\n",
              "                                                  output  generated  \n",
              "0         Massachusetts, Massachusetts: where God is God          0  \n",
              "1      Café Coffee Day, We’re Brewing A Marathon Of C...          0  \n",
              "2                        Colorado, Rocky Mountain Empire          0  \n",
              "3                          Lipton, Start something good.          0  \n",
              "4                  Boost, Boost Guarana: One Step Ahead.          0  \n",
              "...                                                  ...        ...  \n",
              "11613             Disney Junior, Where the Magic Begins!          1  \n",
              "11614                       Intel, Sponsors of tomorrow.          1  \n",
              "11615        Emirates, Fly the airline of the year 2005.          1  \n",
              "11616  Woodford Reserve, Woodford Reserve Reclass <co...          1  \n",
              "11617  Café Hag, You can't knock them over a Cold. Tr...          1  \n",
              "\n",
              "[11618 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c2c1a472-f9fa-4807-afc6-07df1b54699a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>company</th>\n",
              "      <th>category</th>\n",
              "      <th>slogan</th>\n",
              "      <th>language</th>\n",
              "      <th>slogan_masked</th>\n",
              "      <th>prompt</th>\n",
              "      <th>output</th>\n",
              "      <th>generated</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Massachusetts</td>\n",
              "      <td>Tours</td>\n",
              "      <td>Massachusetts: where God is God</td>\n",
              "      <td>en</td>\n",
              "      <td>&lt;company&gt;: where God is God</td>\n",
              "      <td>Massachusetts, Tours</td>\n",
              "      <td>Massachusetts, Massachusetts: where God is God</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Café Coffee Day</td>\n",
              "      <td>Drinking</td>\n",
              "      <td>We’re Brewing A Marathon Of Celebrations This ...</td>\n",
              "      <td>en</td>\n",
              "      <td>We’re Brewing A Marathon Of Celebrations This ...</td>\n",
              "      <td>Café Coffee Day, Drinking</td>\n",
              "      <td>Café Coffee Day, We’re Brewing A Marathon Of C...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Colorado</td>\n",
              "      <td>Tours</td>\n",
              "      <td>Rocky Mountain Empire</td>\n",
              "      <td>en</td>\n",
              "      <td>Rocky Mountain Empire</td>\n",
              "      <td>Colorado, Tours</td>\n",
              "      <td>Colorado, Rocky Mountain Empire</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Lipton</td>\n",
              "      <td>Drinking</td>\n",
              "      <td>Start something good.</td>\n",
              "      <td>en</td>\n",
              "      <td>Start something good.</td>\n",
              "      <td>Lipton, Drinking</td>\n",
              "      <td>Lipton, Start something good.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Boost</td>\n",
              "      <td>Food</td>\n",
              "      <td>Boost Guarana: One Step Ahead.</td>\n",
              "      <td>en</td>\n",
              "      <td>&lt;company&gt; Guarana: One Step Ahead.</td>\n",
              "      <td>Boost, Food</td>\n",
              "      <td>Boost, Boost Guarana: One Step Ahead.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11613</th>\n",
              "      <td>Disney Junior</td>\n",
              "      <td>Business</td>\n",
              "      <td>Welcome to Disney Junior!</td>\n",
              "      <td>en</td>\n",
              "      <td>Welcome to &lt;company&gt;!</td>\n",
              "      <td>Disney Junior, Business</td>\n",
              "      <td>Disney Junior, Where the Magic Begins!</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11614</th>\n",
              "      <td>Intel</td>\n",
              "      <td>Technology</td>\n",
              "      <td>The Computer Inside.</td>\n",
              "      <td>en</td>\n",
              "      <td>The Computer Inside.</td>\n",
              "      <td>Intel, Technology</td>\n",
              "      <td>Intel, Sponsors of tomorrow.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11615</th>\n",
              "      <td>Emirates</td>\n",
              "      <td>Airlines</td>\n",
              "      <td>Fly us once, fly us always</td>\n",
              "      <td>en</td>\n",
              "      <td>Fly us once, fly us always</td>\n",
              "      <td>Emirates, Airlines</td>\n",
              "      <td>Emirates, Fly the airline of the year 2005.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11616</th>\n",
              "      <td>Woodford Reserve</td>\n",
              "      <td>Drinking</td>\n",
              "      <td>Handcrafted Kentucky bourbon.</td>\n",
              "      <td>en</td>\n",
              "      <td>Handcrafted Kentucky bourbon.</td>\n",
              "      <td>Woodford Reserve, Drinking</td>\n",
              "      <td>Woodford Reserve, Woodford Reserve Reclass &lt;co...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11617</th>\n",
              "      <td>Café Hag</td>\n",
              "      <td>Drinking</td>\n",
              "      <td>With coffee this good, who needs caffeine?</td>\n",
              "      <td>en</td>\n",
              "      <td>With coffee this good, who needs caffeine?</td>\n",
              "      <td>Café Hag, Drinking</td>\n",
              "      <td>Café Hag, You can't knock them over a Cold. Tr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11618 rows × 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c2c1a472-f9fa-4807-afc6-07df1b54699a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c2c1a472-f9fa-4807-afc6-07df1b54699a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c2c1a472-f9fa-4807-afc6-07df1b54699a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classifier"
      ],
      "metadata": {
        "id": "kfqPiMIP01r1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Recombine the whole dataset and reshuffle it, then split it again into train, dev, test.\n",
        "6. Drop the `'original'` column for the test set.\n",
        "7. Train a classifier on the columns `'output'` and `'original'`\n",
        "8. Let the classifier guess how many (and which) slogans have been generated by the model."
      ],
      "metadata": {
        "id": "vzI2zGFq16uG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "train_class, dev_class, test_class = np.split(df_class.sample(frac=1), [int(.8*len(df_class)), int(.9*len(df_class))])"
      ],
      "metadata": {
        "id": "zd99a4b95hle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_class.generated.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4eyKxomuB8L",
        "outputId": "9ed7f9ab-d856-4c1d-b481-826b0c576cca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    8382\n",
              "1     912\n",
              "Name: generated, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev_class.generated.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xUTXpIlvA6s",
        "outputId": "aef0d7c4-acd1-4f28-baa3-a42a6e0f0d23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    1036\n",
              "1     126\n",
              "Name: generated, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_class.generated.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkPriL0ZvVZQ",
        "outputId": "993d33e2-895f-41e6-b96d-b9e6cf098ca1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    1038\n",
              "1     124\n",
              "Name: generated, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "\n",
        "tokenizer_class = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model_class = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9pay_WHoNQA",
        "outputId": "119e061c-4594-46dc-cc45-cdf2ff38fc7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_slogan_encodings = tokenizer_class(train_class['output'].tolist(), truncation=True, padding=True)\n",
        "dev_slogan_encodings = tokenizer_class(dev_class['output'].tolist(), truncation=True, padding=True)\n",
        "test_slogan_encodings = tokenizer_class(test_class['output'].tolist(), truncation=True, padding=True)"
      ],
      "metadata": {
        "id": "ULGmjtAjpJ_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class ClassDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset_class = ClassDataset(train_slogan_encodings, train_class['generated'].tolist())\n",
        "dev_dataset_class = ClassDataset(dev_slogan_encodings, dev_class['generated'].tolist())\n",
        "test_dataset_class = ClassDataset(test_slogan_encodings, test_class['generated'].tolist())\n",
        "\n",
        "train_loader_class = DataLoader(train_dataset_class, batch_size=16, shuffle=True)\n",
        "dev_loader_class = DataLoader(dev_dataset_class, batch_size=16, shuffle=True)\n",
        "test_loader_class = DataLoader(test_dataset_class, batch_size=16, shuffle=True)"
      ],
      "metadata": {
        "id": "vIV2TqCroJ8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model_class.to(device)\n",
        "\n",
        "optim = AdamW(model_class.parameters(), lr=5e-5)\n",
        "\n",
        "for epoch in tqdm(range(3)):\n",
        "    model_class.train()\n",
        "    for batch in train_loader_class:\n",
        "        optim.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = model_class(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs[0]\n",
        "        loss.backward()\n",
        "        optim.step()"
      ],
      "metadata": {
        "id": "C5Megymh5y-f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "429c165e-06b4-47d5-dfdc-c6479304e9ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "100%|██████████| 3/3 [03:53<00:00, 77.77s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = '/content/model_class.pt'\n",
        "torch.save(model_class.state_dict(), model_path)"
      ],
      "metadata": {
        "id": "C_RRz25yySoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "model_path = '/content/drive/MyDrive/model_class.pt'\n",
        "state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n",
        "model_class.load_state_dict(state_dict, model_path)\n",
        "device = torch.device('cuda')\n",
        "model_class.to(device)"
      ],
      "metadata": {
        "id": "0qT7q6w42Ieg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate the model"
      ],
      "metadata": {
        "id": "7W7t0yOS39GD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_class.eval()\n",
        "preds_class = []\n",
        "labels_class = []\n",
        "for batch in dev_loader_class:\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model_class(input_ids, attention_mask=attention_mask)\n",
        "    logits = outputs[0]\n",
        "    preds_class.extend(logits.argmax(dim=-1).tolist())\n",
        "    labels_class.extend(batch['labels'].tolist())"
      ],
      "metadata": {
        "id": "f3GrXKnCtaHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "accuracy_class = accuracy_score(labels_class, preds_class)\n",
        "f1_class = precision_recall_fscore_support(labels_class, preds_class, average='weighted')\n",
        "\n",
        "print('Accuracy:', accuracy_class)\n",
        "print('F1 Score:', f1_class)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDAISegNtqe5",
        "outputId": "0ee0bf0d-4551-45b3-f62f-baeede708ae1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8889845094664371\n",
            "F1 Score: (0.8479109398620553, 0.8889845094664371, 0.8532420261931783, None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though the score seems pretty high, we must keep in mind that almost 90% of the slogans in the test set are original, so we expect the model at least to spot them.\n",
        "\n",
        "However the model is not able to predict all the correct labels, which means that our slogan builder performed well."
      ],
      "metadata": {
        "id": "H27Vlb634AU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "cm = confusion_matrix(test_class['generated'], preds_class, labels=train_class['generated'].unique())\n",
        "\n",
        "cm_df = pd.DataFrame(cm, index = train_class['generated'].unique(),\n",
        "                        columns = train_class['generated'].unique())\n",
        "\n",
        "plt.figure(figsize=(10,7))\n",
        "sns.heatmap(cm_df, annot=True, fmt='g')\n",
        "plt.title('BERT base classifier\\nAccuracy: {0:.3f}'.format(accuracy_class))\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "id": "jTtfbxEswoeW",
        "outputId": "795cb6fb-b607-4d05-b930-b4314447c0a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By plotting the confusion matrix we can see that our classifier misclassified almost all generated slogans. In fact, we can see that 121 out 124 generated slogans have been labeled as original while actually they were not (False Positive).\n",
        "\n",
        "Note that the model also predicted 21 False Negative slogans, which means that somehow the model spotted something unregular in the original slogans, which might have been a confounder for generated slogans."
      ],
      "metadata": {
        "id": "r1DFDn7D4b-V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Append the results of the classifier to the test set."
      ],
      "metadata": {
        "id": "Kn4Qq1_m3HYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_class['class'] = preds_class"
      ],
      "metadata": {
        "id": "bEy-NOSx2FYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Show some examples of the classification."
      ],
      "metadata": {
        "id": "l1JazMZG3NCk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When printing the TN, we see that they look pretty credible actually."
      ],
      "metadata": {
        "id": "BPlz-J675GUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_class[(test_class['class']==1)&(test_class['generated']==1)].output.tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppblzBfb3Meh",
        "outputId": "e46d1ede-eee8-4dac-943d-8c6a1981c667"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Nescafe, 95% of all body and sugar drop from caffein.',\n",
              " \"Tetley's Brewery, Simplely Tetley's.\",\n",
              " 'Michigan, Monthly low cost of living in an instant suburb of 100+ cities']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_class[(test_class['class']==1)&(test_class['generated']==0)].output.tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8NqNt_t30vJ",
        "outputId": "dfb19cd1-ec8b-4b6c-afd5-63950ae8795a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Theakston, Cool cask - Real substance.',\n",
              " 'Britannia Industries, A cookie that you canâ\\x80\\x99t stop sharing.\\xa0',\n",
              " \"Cadbury Roses, Say 'Thank You', with Cadbury Roses.\",\n",
              " 'LOT Polish Airlines, You choose the direction',\n",
              " 'DB Schenker, Automotive Powerhouse',\n",
              " 'Florida, Florida: Where America Goes to Die',\n",
              " 'Sauza Tequila, Stay pure.',\n",
              " \"L'Oreal, Two-in-one mascara. Conditions. Lengthens.\",\n",
              " 'KLM Royal Dutch Airlines, You think of KLM freight automatically',\n",
              " \"Harvey's, You know my tastes, Harvey's\",\n",
              " 'Irn-Bru, Diet Irn-Bru. I never knew 4 1⁄2 inches [11 cm] could give so much pleasure.',\n",
              " 'Mountain Dew, Yahoo Mountain Dew… It’ll Tickle Your Innards.',\n",
              " 'Honda, First man, then machine.',\n",
              " 'Listerine, As easy as rinsing. As effective as flossing.',\n",
              " 'WIN Television, Now You’re Talking',\n",
              " \"Ballantine's, Just fine.\",\n",
              " 'Swissair, Our sign is a promise',\n",
              " 'Saint-Gobain, Give your home a new avatar in seven days',\n",
              " \"Buxton, Preserving Wimbledon's purity.\",\n",
              " 'MetLife, Have You Met Life Today?']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_class[test_class['generated']==1].output.tolist()[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UHPopC3zv7_",
        "outputId": "f0d28ffb-f29f-469c-fa4a-165380c8437b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"J2O, There's An Animal in all the world.\",\n",
              " 'Bharti Airtel, Express Yourself.',\n",
              " 'Suzuki, Words don’t do justice but they‘re giving it their shot',\n",
              " \"Dewar's, Science for Dewar's.\",\n",
              " 'Mountain Dew, Mountain Dew. Tickle Mountain Dew.',\n",
              " 'Bosch, Bosch, Bosch the Bosch. We Bosch makes the Bosch',\n",
              " \"Seattle's Best Coffee, A Real Good Clean Feeling In Every Place.\",\n",
              " 'Western Union, Eastern Union, In the Corner, Western Union. To Love It.',\n",
              " 'Jameson Irish Whiskey, Seriously playful.',\n",
              " 'Rhode Island, Small Rhode Island charm',\n",
              " 'Michigan, welcome to Michigan and welcome to Michigan. welcome to Michigan.',\n",
              " \"Welch's, Energise more to life.\",\n",
              " 'Adidas, ',\n",
              " 'Dairyland, Ours is the Answer.',\n",
              " 'Miller Beer, Champagne of Bottled Beers.',\n",
              " 'Berkshire Hathaway, Get off the door. Get on the door. Get on the door',\n",
              " 'Buck Knives, For art lovers like these!',\n",
              " \"Bow Tie Cigar Company, It's A Fun Idea to Print\",\n",
              " 'Black & Decker, Get in. Get out of it.',\n",
              " 'Lea & Perrins, Lea & Perrins. Lea & Perrins']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.to_csv('train_class.csv', sep=\";\")\n",
        "dev.to_csv('dev_class.csv', sep=\";\")\n",
        "test.to_csv('test_class.csv', sep=\";\")"
      ],
      "metadata": {
        "id": "ang24qUO5kMb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
